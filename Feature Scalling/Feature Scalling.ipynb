{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scalling\n",
    "\n",
    "Feature scaling is an essential step in machine learning, particularly for algorithms that are sensitive to the magnitude of features. It ensures that all features contribute equally to the model's performance, preventing bias due to differing scales. Here’s why feature scaling is important\n",
    "\n",
    "01. Prevents Dominance of Large-Scale Features <br/>\n",
    " - Many datasets have features with different units (e.g., age in years vs. income in dollars).\n",
    " - Without scaling, features with larger magnitudes can dominate those with smaller values, leading to biased models. <br/>\n",
    "    Example: In a salary prediction model, if age is between 20-60 years but income is in thousands, income will heavily influence the model.\n",
    "02. Required for Distance-Based Algorithm <br/>\n",
    " - Algorithms like KNN, K-Means, SVM, and PCA rely on distance metrics (Euclidean distance, Manhattan distance). <br/>\n",
    "\n",
    "<br/>\n",
    "\n",
    "When Is Feature Scaling Not Needed? <br/>\n",
    "Tree-based models (e.g., Decision Trees, Random Forest, XGBoost) are not affected by scaling because they use conditions like if X > threshold rather than distances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.array([[26,50000],\n",
    "                 [29,70000],\n",
    "                 [34,55000],\n",
    "                 [31,41000],])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization rescales feature values to a fixed range, usually [0, 1] or [-1, 1]. <br/>\n",
    "\n",
    "When to Use Normalization? <br/>\n",
    "✔ When features have different scales (e.g., age vs. salary). <br/>\n",
    "✔ When the data has a bounded range and follows a uniform or non-Gaussian distribution. <br/>\n",
    "✔ Suitable for distance-based algorithms like KNN, K-Means, SVM, and Neural Networks (since weights adjust better with normalized inputs). <br/>\n",
    "\n",
    "Example Use Case: <br/>\n",
    "If the temperature feature ranges from 0°C to 100°C and the humidity feature ranges from 0% to 1%, normalization helps bring them to the same scale.\n",
    "<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.31034483],\n",
       "       [0.375     , 1.        ],\n",
       "       [1.        , 0.48275862],\n",
       "       [0.625     , 0.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data);\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization\n",
    "\n",
    "Standardization transforms features to have a mean of 0 and a standard deviation of 1, making the data follow a normal distribution <br/>\n",
    "\n",
    "When to Use Standardization? <br/>\n",
    "✔ When the data follows a normal (Gaussian) distribution. <br/>\n",
    "✔ When features have different scales, but we want to maintain outliers and natural data spread. <br/>\n",
    "✔ Suitable for linear regression, logistic regression, PCA, and gradient descent-based models. <br/>\n",
    "✔ Useful for SVM and k-means clustering, where distance metrics matter but need robust scaling. <br/>\n",
    "\n",
    "If the dataset contains features like height (cm), weight (kg), and age (years), standardization ensures they have comparable influence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.37198868, -0.3805212 ],\n",
       "       [-0.34299717,  1.52208478],\n",
       "       [ 1.37198868,  0.0951303 ],\n",
       "       [ 0.34299717, -1.23669388]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data);\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
